2024-10-05T22:22:16 | INFO | vindlu : Logging to: scripts/videochat_mistral/train.log
2024-10-05T22:22:16 | INFO | utils.config_utils : config: {
  anno_root_it: .
  available_corpus: {
      caption_HumanML3D: ['./HumanML3D/train.json', './HumanML3D', 'video']
      videochat2_instruction: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_new: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_hd: [['./HumanML3D/train.json', './HumanML3D', 'video']] }
  train_corpus: videochat2_instruction_new
  train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
  test_file: {

  test_types: []
  num_workers: 6
  stop_key: None
  num_frames: 20
  num_frames_test: 20
  batch_size: 1
  max_txt_l: 512
  pre_text: False
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 20
          sample_type: rand
          num_frames_test: 20
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 512
          video: 512 }
      batch_size: {
          image: 1
          video: 1 }
      batch_size_test: {
          image: 1
          video: 1 } }
  model: {
      model_cls: VideoChat2_it_mistral
      vit_blip_model_path: ./UMT/umt_l16_qformer.pth
      mistral_model_path: ./Mistral
      videochat2_model_path: ./ckpt/stage3/videochat2_mistral_7b_stage3.pth
      freeze_vit: False
      freeze_qformer: False
      max_txt_len: 512
      low_resource: False
      vision_encoder: {
          name: vit_l14
          img_size: 224
          patch_size: 16
          d_model: 1024
          encoder_embed_dim: 1024
          encoder_depth: 24
          encoder_num_heads: 16
          drop_path_rate: 0.0
          num_frames: 20
          tubelet_size: 1
          use_checkpoint: True
          checkpoint_num: 18
          pretrained: 
          return_index: -2
          vit_add_ln: True
          ckpt_num_frame: 4 }
      num_query_token: 32
      qformer_hidden_dropout_prob: 0.1
      qformer_attention_probs_dropout_prob: 0.1
      qformer_drop_path_rate: 0.2
      extra_num_query_token: 64
      qformer_text_input: True
      system: 
      start_token: <Video>
      end_token: </Video>
      add_second_msg: True
      img_start_token: <Image>
      img_end_token: </Image>
      random_shuffle: True
      use_flash_attention: True
      use_lora: True
      lora_r: 16
      lora_alpha: 32
      lora_dropout: 0.1 }
  optimizer: {
      opt: adamW
      lr: 2e-05
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 3
      min_lr_multi: 0.25
      warmup_epochs: 0.6 }
  evaluate: False
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: likunchang
      project: videochat2 }
  dist_url: env://
  device: cuda:0
  mode: it_mistral
  output_dir: scripts/videochat_mistral/
  resume: False
  debug: False
  log_freq: 10
  seed: 42
  save_latest: True
  auto_resume: True
  pretrained_path: 
  rank: 0
  world_size: 1
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-10-05T22:22:16 | INFO | __main__ : train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
2024-10-05T22:22:16 | INFO | __main__ : Creating dataset for it_mistral
2024-10-05T22:22:16 | INFO | dataset.it_dataset_mistral : Load json file
2024-10-05T22:24:26 | INFO | vindlu : Logging to: scripts/videochat_mistral/train.log
2024-10-05T22:24:26 | INFO | utils.config_utils : config: {
  anno_root_it: .
  available_corpus: {
      caption_HumanML3D: ['./HumanML3D/train.json', './HumanML3D', 'video']
      videochat2_instruction: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_new: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_hd: [['./HumanML3D/train.json', './HumanML3D', 'video']] }
  train_corpus: videochat2_instruction_new
  train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
  test_file: {

  test_types: []
  num_workers: 6
  stop_key: None
  num_frames: 20
  num_frames_test: 20
  batch_size: 1
  max_txt_l: 512
  pre_text: False
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 20
          sample_type: rand
          num_frames_test: 20
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 512
          video: 512 }
      batch_size: {
          image: 1
          video: 1 }
      batch_size_test: {
          image: 1
          video: 1 } }
  model: {
      model_cls: VideoChat2_it_mistral
      vit_blip_model_path: ./UMT/umt_l16_qformer.pth
      mistral_model_path: ./Mistral
      videochat2_model_path: ./ckpt/stage3/videochat2_mistral_7b_stage3.pth
      freeze_vit: False
      freeze_qformer: False
      max_txt_len: 512
      low_resource: False
      vision_encoder: {
          name: vit_l14
          img_size: 224
          patch_size: 16
          d_model: 1024
          encoder_embed_dim: 1024
          encoder_depth: 24
          encoder_num_heads: 16
          drop_path_rate: 0.0
          num_frames: 20
          tubelet_size: 1
          use_checkpoint: True
          checkpoint_num: 18
          pretrained: 
          return_index: -2
          vit_add_ln: True
          ckpt_num_frame: 4 }
      num_query_token: 32
      qformer_hidden_dropout_prob: 0.1
      qformer_attention_probs_dropout_prob: 0.1
      qformer_drop_path_rate: 0.2
      extra_num_query_token: 64
      qformer_text_input: True
      system: 
      start_token: <Video>
      end_token: </Video>
      add_second_msg: True
      img_start_token: <Image>
      img_end_token: </Image>
      random_shuffle: True
      use_flash_attention: True
      use_lora: True
      lora_r: 16
      lora_alpha: 32
      lora_dropout: 0.1 }
  optimizer: {
      opt: adamW
      lr: 2e-05
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 3
      min_lr_multi: 0.25
      warmup_epochs: 0.6 }
  evaluate: False
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: likunchang
      project: videochat2 }
  dist_url: env://
  device: cuda:0
  mode: it_mistral
  output_dir: scripts/videochat_mistral/
  resume: False
  debug: False
  log_freq: 10
  seed: 42
  save_latest: True
  auto_resume: True
  pretrained_path: 
  rank: 0
  world_size: 1
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-10-05T22:24:26 | INFO | __main__ : train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
2024-10-05T22:24:26 | INFO | __main__ : Creating dataset for it_mistral
2024-10-05T22:24:26 | INFO | dataset.it_dataset_mistral : Load json file
2024-10-05T22:24:26 | INFO | dataset.it_dataset_mistral : Random shuffle: True
2024-10-05T22:24:26 | INFO | dataset.it_dataset_mistral : Return question with instruction: False
2024-10-05T22:24:26 | INFO | dataset.it_dataset_mistral : Use decord for data in ['./HumanML3D/train.json', './HumanML3D', 'video']
2024-10-05T22:24:26 | INFO | tasks.shared_utils : Creating model
2024-10-05T22:24:26 | INFO | models.videochat_mistra.videochat2_it_mistral : Add instruction in qformer: True
2024-10-05T22:27:20 | INFO | vindlu : Logging to: scripts/videochat_mistral/train.log
2024-10-05T22:27:20 | INFO | utils.config_utils : config: {
  anno_root_it: .
  available_corpus: {
      caption_HumanML3D: ['./HumanML3D/train.json', './HumanML3D', 'video']
      videochat2_instruction: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_new: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_hd: [['./HumanML3D/train.json', './HumanML3D', 'video']] }
  train_corpus: videochat2_instruction_new
  train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
  test_file: {

  test_types: []
  num_workers: 6
  stop_key: None
  num_frames: 20
  num_frames_test: 20
  batch_size: 1
  max_txt_l: 512
  pre_text: False
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 20
          sample_type: rand
          num_frames_test: 20
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 512
          video: 512 }
      batch_size: {
          image: 1
          video: 1 }
      batch_size_test: {
          image: 1
          video: 1 } }
  model: {
      model_cls: VideoChat2_it_mistral
      vit_blip_model_path: ./UMT/umt_l16_qformer.pth
      mistral_model_path: ./Mistral
      videochat2_model_path: ./ckpt/stage3/videochat2_mistral_7b_stage3.pth
      freeze_vit: False
      freeze_qformer: False
      max_txt_len: 512
      low_resource: False
      vision_encoder: {
          name: vit_l14
          img_size: 224
          patch_size: 16
          d_model: 1024
          encoder_embed_dim: 1024
          encoder_depth: 24
          encoder_num_heads: 16
          drop_path_rate: 0.0
          num_frames: 20
          tubelet_size: 1
          use_checkpoint: True
          checkpoint_num: 18
          pretrained: 
          return_index: -2
          vit_add_ln: True
          ckpt_num_frame: 4 }
      num_query_token: 32
      qformer_hidden_dropout_prob: 0.1
      qformer_attention_probs_dropout_prob: 0.1
      qformer_drop_path_rate: 0.2
      extra_num_query_token: 64
      qformer_text_input: True
      system: 
      start_token: <Video>
      end_token: </Video>
      add_second_msg: True
      img_start_token: <Image>
      img_end_token: </Image>
      random_shuffle: True
      use_flash_attention: True
      use_lora: True
      lora_r: 16
      lora_alpha: 32
      lora_dropout: 0.1 }
  optimizer: {
      opt: adamW
      lr: 2e-05
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 3
      min_lr_multi: 0.25
      warmup_epochs: 0.6 }
  evaluate: False
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: likunchang
      project: videochat2 }
  dist_url: env://
  device: cuda:0
  mode: it_mistral
  output_dir: scripts/videochat_mistral/
  resume: False
  debug: False
  log_freq: 10
  seed: 42
  save_latest: True
  auto_resume: True
  pretrained_path: 
  rank: 0
  world_size: 1
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-10-05T22:27:20 | INFO | __main__ : train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
2024-10-05T22:27:20 | INFO | __main__ : Creating dataset for it_mistral
2024-10-05T22:27:20 | INFO | dataset.it_dataset_mistral : Load json file
2024-10-05T22:27:20 | INFO | dataset.it_dataset_mistral : Random shuffle: True
2024-10-05T22:27:20 | INFO | dataset.it_dataset_mistral : Return question with instruction: False
2024-10-05T22:27:20 | INFO | dataset.it_dataset_mistral : Use decord for data in ['./HumanML3D/train.json', './HumanML3D', 'video']
2024-10-05T22:27:20 | INFO | tasks.shared_utils : Creating model
2024-10-05T22:27:20 | INFO | models.videochat_mistra.videochat2_it_mistral : Add instruction in qformer: True
2024-10-05T22:27:20 | INFO | models.blip2.vit : Num of patches: 3920
2024-10-05T22:27:20 | INFO | models.blip2.vit : Use checkpoint: True
2024-10-05T22:27:20 | INFO | models.blip2.vit : Checkpoint number: 18
2024-10-05T22:27:20 | INFO | models.blip2.vit : Real runing depth: 23
2024-10-05T22:27:20 | INFO | models.blip2.vit : Interpolate position embedding
2024-10-05T22:27:20 | INFO | models.blip2.vit : Testing frame: 20
2024-10-05T22:27:20 | INFO | models.blip2.vit : Checkpoint frame: 4
2024-10-05T22:27:23 | INFO | models.blip2.vit : With LN: False
2024-10-05T22:27:23 | INFO | models.blip2.vit : Total 24 layer
2024-10-05T22:27:23 | INFO | models.blip2.vit : Return 23-th layer
2024-10-05T22:27:24 | INFO | models.blip2.vit : No pretrained weights!!!
2024-10-05T22:27:24 | INFO | models.blip2.blip2 : Drop_path:[0.0, 0.0181818176060915, 0.036363635212183, 0.05454545468091965, 0.072727270424366, 0.09090908616781235, 0.10909091681241989, 0.12727272510528564, 0.1454545557498932, 0.16363637149333954, 0.1818181872367859, 0.20000000298023224]
2024-10-05T22:27:24 | INFO | models.blip2.blip2 : BertConfig {
  "add_cross_attention": true,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "cross_attention_freq": 2,
  "drop_path_list": [
    0.0,
    0.0181818176060915,
    0.036363635212183,
    0.05454545468091965,
    0.072727270424366,
    0.09090908616781235,
    0.10909091681241989,
    0.12727272510528564,
    0.1454545557498932,
    0.16363637149333954,
    0.1818181872367859,
    0.20000000298023224
  ],
  "encoder_width": 1024,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "query_length": 32,
  "transformers_version": "4.40.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2024-10-05T22:27:26 | INFO | models.videochat_mistra.videochat2_it_mistral : Load ViT and QFormer from ./UMT/umt_l16_qformer.pth
2024-10-05T22:27:27 | INFO | models.videochat_mistra.videochat2_it_mistral : _IncompatibleKeys(missing_keys=[], unexpected_keys=['vision_temp_embed', 'temp', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias'])
2024-10-05T22:27:27 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading ViT and Q-Former Done
2024-10-05T22:27:27 | INFO | models.videochat_mistra.videochat2_it_mistral : Add extra 64 tokens in QFormer
2024-10-05T22:27:27 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading Mistral
2024-10-05T22:27:27 | INFO | models.videochat_mistra.videochat2_it_mistral : Set pad_token
2024-10-05T22:27:29 | INFO | models.videochat_mistra.videochat2_it_mistral : freeze Mistral
2024-10-05T22:27:29 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading Mistral Done
2024-10-05T22:27:29 | INFO | models.videochat_mistra.videochat2_it_mistral : Use lora
2024-10-05T22:28:30 | INFO | models.videochat_mistra.videochat2_it_mistral : Load VideoChat2 from: ./ckpt/stage3/videochat2_mistral_7b_stage3.pth
2024-10-05T22:29:00 | INFO | vindlu : Logging to: scripts/videochat_mistral/train.log
2024-10-05T22:29:00 | INFO | utils.config_utils : config: {
  anno_root_it: .
  available_corpus: {
      caption_HumanML3D: ['./HumanML3D/train.json', './HumanML3D', 'video']
      videochat2_instruction: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_new: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_hd: [['./HumanML3D/train.json', './HumanML3D', 'video']] }
  train_corpus: videochat2_instruction_new
  train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
  test_file: {

  test_types: []
  num_workers: 6
  stop_key: None
  num_frames: 20
  num_frames_test: 20
  batch_size: 1
  max_txt_l: 512
  pre_text: False
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 20
          sample_type: rand
          num_frames_test: 20
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 512
          video: 512 }
      batch_size: {
          image: 1
          video: 1 }
      batch_size_test: {
          image: 1
          video: 1 } }
  model: {
      model_cls: VideoChat2_it_mistral
      vit_blip_model_path: ./UMT/umt_l16_qformer.pth
      mistral_model_path: ./Mistral
      videochat2_model_path: ./ckpt/stage4/videochat2_hd_mistral_7b_stage4.pth
      freeze_vit: False
      freeze_qformer: False
      max_txt_len: 512
      low_resource: False
      vision_encoder: {
          name: vit_l14
          img_size: 224
          patch_size: 16
          d_model: 1024
          encoder_embed_dim: 1024
          encoder_depth: 24
          encoder_num_heads: 16
          drop_path_rate: 0.0
          num_frames: 20
          tubelet_size: 1
          use_checkpoint: True
          checkpoint_num: 18
          pretrained: 
          return_index: -2
          vit_add_ln: True
          ckpt_num_frame: 4 }
      num_query_token: 32
      qformer_hidden_dropout_prob: 0.1
      qformer_attention_probs_dropout_prob: 0.1
      qformer_drop_path_rate: 0.2
      extra_num_query_token: 64
      qformer_text_input: True
      system: 
      start_token: <Video>
      end_token: </Video>
      add_second_msg: True
      img_start_token: <Image>
      img_end_token: </Image>
      random_shuffle: True
      use_flash_attention: True
      use_lora: True
      lora_r: 16
      lora_alpha: 32
      lora_dropout: 0.1 }
  optimizer: {
      opt: adamW
      lr: 2e-05
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 3
      min_lr_multi: 0.25
      warmup_epochs: 0.6 }
  evaluate: False
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: likunchang
      project: videochat2 }
  dist_url: env://
  device: cuda:0
  mode: it_mistral
  output_dir: scripts/videochat_mistral/
  resume: False
  debug: False
  log_freq: 10
  seed: 42
  save_latest: True
  auto_resume: True
  pretrained_path: 
  rank: 0
  world_size: 1
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-10-05T22:29:00 | INFO | __main__ : train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
2024-10-05T22:29:00 | INFO | __main__ : Creating dataset for it_mistral
2024-10-05T22:29:00 | INFO | dataset.it_dataset_mistral : Load json file
2024-10-05T22:29:00 | INFO | dataset.it_dataset_mistral : Random shuffle: True
2024-10-05T22:29:00 | INFO | dataset.it_dataset_mistral : Return question with instruction: False
2024-10-05T22:29:00 | INFO | dataset.it_dataset_mistral : Use decord for data in ['./HumanML3D/train.json', './HumanML3D', 'video']
2024-10-05T22:29:00 | INFO | tasks.shared_utils : Creating model
2024-10-05T22:29:00 | INFO | models.videochat_mistra.videochat2_it_mistral : Add instruction in qformer: True
2024-10-05T22:29:01 | INFO | models.blip2.vit : Num of patches: 3920
2024-10-05T22:29:01 | INFO | models.blip2.vit : Use checkpoint: True
2024-10-05T22:29:01 | INFO | models.blip2.vit : Checkpoint number: 18
2024-10-05T22:29:01 | INFO | models.blip2.vit : Real runing depth: 23
2024-10-05T22:29:01 | INFO | models.blip2.vit : Interpolate position embedding
2024-10-05T22:29:01 | INFO | models.blip2.vit : Testing frame: 20
2024-10-05T22:29:01 | INFO | models.blip2.vit : Checkpoint frame: 4
2024-10-05T22:29:03 | INFO | models.blip2.vit : With LN: False
2024-10-05T22:29:03 | INFO | models.blip2.vit : Total 24 layer
2024-10-05T22:29:03 | INFO | models.blip2.vit : Return 23-th layer
2024-10-05T22:29:04 | INFO | models.blip2.vit : No pretrained weights!!!
2024-10-05T22:29:04 | INFO | models.blip2.blip2 : Drop_path:[0.0, 0.0181818176060915, 0.036363635212183, 0.05454545468091965, 0.072727270424366, 0.09090908616781235, 0.10909091681241989, 0.12727272510528564, 0.1454545557498932, 0.16363637149333954, 0.1818181872367859, 0.20000000298023224]
2024-10-05T22:29:04 | INFO | models.blip2.blip2 : BertConfig {
  "add_cross_attention": true,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "cross_attention_freq": 2,
  "drop_path_list": [
    0.0,
    0.0181818176060915,
    0.036363635212183,
    0.05454545468091965,
    0.072727270424366,
    0.09090908616781235,
    0.10909091681241989,
    0.12727272510528564,
    0.1454545557498932,
    0.16363637149333954,
    0.1818181872367859,
    0.20000000298023224
  ],
  "encoder_width": 1024,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "query_length": 32,
  "transformers_version": "4.40.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2024-10-05T22:29:07 | INFO | models.videochat_mistra.videochat2_it_mistral : Load ViT and QFormer from ./UMT/umt_l16_qformer.pth
2024-10-05T22:29:07 | INFO | models.videochat_mistra.videochat2_it_mistral : _IncompatibleKeys(missing_keys=[], unexpected_keys=['vision_temp_embed', 'temp', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias'])
2024-10-05T22:29:07 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading ViT and Q-Former Done
2024-10-05T22:29:07 | INFO | models.videochat_mistra.videochat2_it_mistral : Add extra 64 tokens in QFormer
2024-10-05T22:29:07 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading Mistral
2024-10-05T22:29:07 | INFO | models.videochat_mistra.videochat2_it_mistral : Set pad_token
2024-10-05T22:29:09 | INFO | models.videochat_mistra.videochat2_it_mistral : freeze Mistral
2024-10-05T22:29:09 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading Mistral Done
2024-10-05T22:29:09 | INFO | models.videochat_mistra.videochat2_it_mistral : Use lora
2024-10-05T22:30:11 | INFO | models.videochat_mistra.videochat2_it_mistral : Load VideoChat2 from: ./ckpt/stage4/videochat2_hd_mistral_7b_stage4.pth
2024-10-05T22:30:11 | INFO | models.videochat_mistra.videochat2_it_mistral : _IncompatibleKeys(missing_keys=['mistral_model.base_model.model.model.embed_tokens.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.0.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.1.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.2.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.3.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.4.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.5.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.6.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.7.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.8.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.9.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.10.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.11.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.12.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.13.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.14.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.15.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.16.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.17.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.18.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.19.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.20.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.21.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.22.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.23.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.24.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.25.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.26.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.27.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.28.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.29.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.30.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.31.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.norm.weight', 'mistral_model.base_model.model.lm_head.weight'], unexpected_keys=[])
2024-10-05T22:30:14 | INFO | utils.optimizer : diff_names: [], diff_lr: None
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.query_tokens: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.extra_query_tokens: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_layernorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.vision_layernorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.embeddings.word_embeddings.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.embeddings.position_embeddings.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.embeddings.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.embeddings.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:14 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_model.base_model.model.lm_head.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_model.base_model.model.lm_head.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : param module.mistral_proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:30:15 | INFO | utils.optimizer : optimizer -- lr=2e-05 wd=0.02 len(p)=668
2024-10-05T22:30:15 | INFO | utils.optimizer : optimizer -- lr=2e-05 wd=0 len(p)=417
2024-10-05T22:30:15 | INFO | tasks.shared_utils : Auto resuming
2024-10-05T22:30:15 | INFO | tasks.shared_utils : Not found checkpoint in scripts/videochat_mistral/
2024-10-05T22:30:15 | WARNING | tasks.shared_utils : No pretrained checkpoint provided, training from scratch
2024-10-05T22:30:15 | INFO | __main__ : Start training
2024-10-05T22:30:15 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 22 batches in total
dataloader index=0 name=video, batch-size=1 length(#batches)=22 
2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-10-05T22:30:15 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-10-05T22:31:04 | INFO | vindlu : Logging to: scripts/videochat_mistral/train.log
2024-10-05T22:31:04 | INFO | utils.config_utils : config: {
  anno_root_it: .
  available_corpus: {
      caption_HumanML3D: ['./HumanML3D/train.json', './HumanML3D', 'video']
      videochat2_instruction: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_new: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_hd: [['./HumanML3D/train.json', './HumanML3D', 'video']] }
  train_corpus: videochat2_instruction_new
  train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
  test_file: {

  test_types: []
  num_workers: 6
  stop_key: None
  num_frames: 20
  num_frames_test: 20
  batch_size: 1
  max_txt_l: 512
  pre_text: False
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 20
          sample_type: rand
          num_frames_test: 20
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 512
          video: 512 }
      batch_size: {
          image: 1
          video: 1 }
      batch_size_test: {
          image: 1
          video: 1 } }
  model: {
      model_cls: VideoChat2_it_mistral
      vit_blip_model_path: ./UMT/umt_l16_qformer.pth
      mistral_model_path: ./Mistral
      videochat2_model_path: ./ckpt/stage4/videochat2_hd_mistral_7b_stage4.pth
      freeze_vit: False
      freeze_qformer: False
      max_txt_len: 512
      low_resource: False
      vision_encoder: {
          name: vit_l14
          img_size: 224
          patch_size: 16
          d_model: 1024
          encoder_embed_dim: 1024
          encoder_depth: 24
          encoder_num_heads: 16
          drop_path_rate: 0.0
          num_frames: 20
          tubelet_size: 1
          use_checkpoint: True
          checkpoint_num: 18
          pretrained: 
          return_index: -2
          vit_add_ln: True
          ckpt_num_frame: 4 }
      num_query_token: 32
      qformer_hidden_dropout_prob: 0.1
      qformer_attention_probs_dropout_prob: 0.1
      qformer_drop_path_rate: 0.2
      extra_num_query_token: 64
      qformer_text_input: True
      system: 
      start_token: <Video>
      end_token: </Video>
      add_second_msg: True
      img_start_token: <Image>
      img_end_token: </Image>
      random_shuffle: True
      use_flash_attention: True
      use_lora: True
      lora_r: 16
      lora_alpha: 32
      lora_dropout: 0.1 }
  optimizer: {
      opt: adamW
      lr: 2e-05
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 3
      min_lr_multi: 0.25
      warmup_epochs: 0.6 }
  evaluate: False
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: likunchang
      project: videochat2 }
  dist_url: env://
  device: cuda:0
  mode: it_mistral
  output_dir: scripts/videochat_mistral/
  resume: False
  debug: False
  log_freq: 10
  seed: 42
  save_latest: True
  auto_resume: True
  pretrained_path: 
  rank: 0
  world_size: 2
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-10-05T22:31:04 | INFO | __main__ : train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
2024-10-05T22:31:04 | INFO | __main__ : Creating dataset for it_mistral
2024-10-05T22:31:04 | INFO | dataset.it_dataset_mistral : Load json file
2024-10-05T22:31:04 | INFO | dataset.it_dataset_mistral : Random shuffle: True
2024-10-05T22:31:04 | INFO | dataset.it_dataset_mistral : Return question with instruction: False
2024-10-05T22:31:04 | INFO | dataset.it_dataset_mistral : Use decord for data in ['./HumanML3D/train.json', './HumanML3D', 'video']
2024-10-05T22:31:04 | INFO | tasks.shared_utils : Creating model
2024-10-05T22:31:04 | INFO | models.videochat_mistra.videochat2_it_mistral : Add instruction in qformer: True
2024-10-05T22:31:05 | INFO | models.blip2.vit : Num of patches: 3920
2024-10-05T22:31:05 | INFO | models.blip2.vit : Use checkpoint: True
2024-10-05T22:31:05 | INFO | models.blip2.vit : Checkpoint number: 18
2024-10-05T22:31:05 | INFO | models.blip2.vit : Real runing depth: 23
2024-10-05T22:31:05 | INFO | models.blip2.vit : Interpolate position embedding
2024-10-05T22:31:05 | INFO | models.blip2.vit : Testing frame: 20
2024-10-05T22:31:05 | INFO | models.blip2.vit : Checkpoint frame: 4
2024-10-05T22:31:07 | INFO | models.blip2.vit : With LN: False
2024-10-05T22:31:07 | INFO | models.blip2.vit : Total 24 layer
2024-10-05T22:31:07 | INFO | models.blip2.vit : Return 23-th layer
2024-10-05T22:31:09 | INFO | models.blip2.vit : No pretrained weights!!!
2024-10-05T22:31:09 | INFO | models.blip2.blip2 : Drop_path:[0.0, 0.0181818176060915, 0.036363635212183, 0.05454545468091965, 0.072727270424366, 0.09090908616781235, 0.10909091681241989, 0.12727272510528564, 0.1454545557498932, 0.16363637149333954, 0.1818181872367859, 0.20000000298023224]
2024-10-05T22:31:09 | INFO | models.blip2.blip2 : BertConfig {
  "add_cross_attention": true,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "cross_attention_freq": 2,
  "drop_path_list": [
    0.0,
    0.0181818176060915,
    0.036363635212183,
    0.05454545468091965,
    0.072727270424366,
    0.09090908616781235,
    0.10909091681241989,
    0.12727272510528564,
    0.1454545557498932,
    0.16363637149333954,
    0.1818181872367859,
    0.20000000298023224
  ],
  "encoder_width": 1024,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "query_length": 32,
  "transformers_version": "4.40.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2024-10-05T22:31:11 | INFO | models.videochat_mistra.videochat2_it_mistral : Load ViT and QFormer from ./UMT/umt_l16_qformer.pth
2024-10-05T22:31:12 | INFO | models.videochat_mistra.videochat2_it_mistral : _IncompatibleKeys(missing_keys=[], unexpected_keys=['vision_temp_embed', 'temp', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias'])
2024-10-05T22:31:12 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading ViT and Q-Former Done
2024-10-05T22:31:12 | INFO | models.videochat_mistra.videochat2_it_mistral : Add extra 64 tokens in QFormer
2024-10-05T22:31:12 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading Mistral
2024-10-05T22:31:12 | INFO | models.videochat_mistra.videochat2_it_mistral : Set pad_token
2024-10-05T22:31:20 | INFO | models.videochat_mistra.videochat2_it_mistral : freeze Mistral
2024-10-05T22:31:20 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading Mistral Done
2024-10-05T22:31:20 | INFO | models.videochat_mistra.videochat2_it_mistral : Use lora
2024-10-05T22:32:25 | INFO | models.videochat_mistra.videochat2_it_mistral : Load VideoChat2 from: ./ckpt/stage4/videochat2_hd_mistral_7b_stage4.pth
2024-10-05T22:32:26 | INFO | models.videochat_mistra.videochat2_it_mistral : _IncompatibleKeys(missing_keys=['mistral_model.base_model.model.model.embed_tokens.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.0.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.1.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.2.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.3.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.4.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.5.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.6.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.7.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.8.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.9.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.10.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.11.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.12.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.13.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.14.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.15.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.16.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.17.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.18.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.19.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.20.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.21.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.22.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.23.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.24.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.25.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.26.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.27.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.28.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.29.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.30.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.31.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.norm.weight', 'mistral_model.base_model.model.lm_head.weight'], unexpected_keys=[])
2024-10-05T22:38:11 | INFO | vindlu : Logging to: scripts/videochat_mistral/train.log
2024-10-05T22:38:11 | INFO | utils.config_utils : config: {
  anno_root_it: .
  available_corpus: {
      caption_HumanML3D: ['./HumanML3D/train.json', './HumanML3D', 'video']
      videochat2_instruction: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_new: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_hd: [['./HumanML3D/train.json', './HumanML3D', 'video']] }
  train_corpus: videochat2_instruction_new
  train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
  test_file: {

  test_types: []
  num_workers: 6
  stop_key: None
  num_frames: 20
  num_frames_test: 20
  batch_size: 1
  max_txt_l: 512
  pre_text: False
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 20
          sample_type: rand
          num_frames_test: 20
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 512
          video: 512 }
      batch_size: {
          image: 1
          video: 1 }
      batch_size_test: {
          image: 1
          video: 1 } }
  model: {
      model_cls: VideoChat2_it_mistral
      vit_blip_model_path: ./UMT/umt_l16_qformer.pth
      mistral_model_path: ./Mistral
      videochat2_model_path: ./ckpt/stage2/videochat2_mistral_7b_stage2.pth
      freeze_vit: False
      freeze_qformer: False
      max_txt_len: 512
      low_resource: False
      vision_encoder: {
          name: vit_l14
          img_size: 224
          patch_size: 16
          d_model: 1024
          encoder_embed_dim: 1024
          encoder_depth: 24
          encoder_num_heads: 16
          drop_path_rate: 0.0
          num_frames: 20
          tubelet_size: 1
          use_checkpoint: True
          checkpoint_num: 18
          pretrained: 
          return_index: -2
          vit_add_ln: True
          ckpt_num_frame: 4 }
      num_query_token: 32
      qformer_hidden_dropout_prob: 0.1
      qformer_attention_probs_dropout_prob: 0.1
      qformer_drop_path_rate: 0.2
      extra_num_query_token: 64
      qformer_text_input: True
      system: 
      start_token: <Video>
      end_token: </Video>
      add_second_msg: True
      img_start_token: <Image>
      img_end_token: </Image>
      random_shuffle: True
      use_flash_attention: True
      use_lora: True
      lora_r: 16
      lora_alpha: 32
      lora_dropout: 0.1 }
  optimizer: {
      opt: adamW
      lr: 2e-05
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 3
      min_lr_multi: 0.25
      warmup_epochs: 0.6 }
  evaluate: False
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: likunchang
      project: videochat2 }
  dist_url: env://
  device: cuda:0
  mode: it_mistral
  output_dir: scripts/videochat_mistral/
  resume: False
  debug: False
  log_freq: 10
  seed: 42
  save_latest: True
  auto_resume: True
  pretrained_path: 
  rank: 0
  world_size: 2
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-10-05T22:38:11 | INFO | __main__ : train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
2024-10-05T22:38:11 | INFO | __main__ : Creating dataset for it_mistral
2024-10-05T22:38:11 | INFO | dataset.it_dataset_mistral : Load json file
2024-10-05T22:38:11 | INFO | dataset.it_dataset_mistral : Random shuffle: True
2024-10-05T22:38:11 | INFO | dataset.it_dataset_mistral : Return question with instruction: False
2024-10-05T22:38:11 | INFO | dataset.it_dataset_mistral : Use decord for data in ['./HumanML3D/train.json', './HumanML3D', 'video']
2024-10-05T22:38:11 | INFO | tasks.shared_utils : Creating model
2024-10-05T22:38:11 | INFO | models.videochat_mistra.videochat2_it_mistral : Add instruction in qformer: True
2024-10-05T22:38:11 | INFO | models.blip2.vit : Num of patches: 3920
2024-10-05T22:38:11 | INFO | models.blip2.vit : Use checkpoint: True
2024-10-05T22:38:11 | INFO | models.blip2.vit : Checkpoint number: 18
2024-10-05T22:38:11 | INFO | models.blip2.vit : Real runing depth: 23
2024-10-05T22:38:11 | INFO | models.blip2.vit : Interpolate position embedding
2024-10-05T22:38:11 | INFO | models.blip2.vit : Testing frame: 20
2024-10-05T22:38:11 | INFO | models.blip2.vit : Checkpoint frame: 4
2024-10-05T22:38:14 | INFO | models.blip2.vit : With LN: False
2024-10-05T22:38:14 | INFO | models.blip2.vit : Total 24 layer
2024-10-05T22:38:14 | INFO | models.blip2.vit : Return 23-th layer
2024-10-05T22:38:15 | INFO | models.blip2.vit : No pretrained weights!!!
2024-10-05T22:38:15 | INFO | models.blip2.blip2 : Drop_path:[0.0, 0.0181818176060915, 0.036363635212183, 0.05454545468091965, 0.072727270424366, 0.09090908616781235, 0.10909091681241989, 0.12727272510528564, 0.1454545557498932, 0.16363637149333954, 0.1818181872367859, 0.20000000298023224]
2024-10-05T22:38:15 | INFO | models.blip2.blip2 : BertConfig {
  "add_cross_attention": true,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "cross_attention_freq": 2,
  "drop_path_list": [
    0.0,
    0.0181818176060915,
    0.036363635212183,
    0.05454545468091965,
    0.072727270424366,
    0.09090908616781235,
    0.10909091681241989,
    0.12727272510528564,
    0.1454545557498932,
    0.16363637149333954,
    0.1818181872367859,
    0.20000000298023224
  ],
  "encoder_width": 1024,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "query_length": 32,
  "transformers_version": "4.40.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2024-10-05T22:38:17 | INFO | models.videochat_mistra.videochat2_it_mistral : Load ViT and QFormer from ./UMT/umt_l16_qformer.pth
2024-10-05T22:38:18 | INFO | models.videochat_mistra.videochat2_it_mistral : _IncompatibleKeys(missing_keys=[], unexpected_keys=['vision_temp_embed', 'temp', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias'])
2024-10-05T22:38:18 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading ViT and Q-Former Done
2024-10-05T22:38:18 | INFO | models.videochat_mistra.videochat2_it_mistral : Add extra 64 tokens in QFormer
2024-10-05T22:38:18 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading Mistral
2024-10-05T22:38:18 | INFO | models.videochat_mistra.videochat2_it_mistral : Set pad_token
2024-10-05T22:38:26 | INFO | models.videochat_mistra.videochat2_it_mistral : freeze Mistral
2024-10-05T22:38:26 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading Mistral Done
2024-10-05T22:38:26 | INFO | models.videochat_mistra.videochat2_it_mistral : Use lora
2024-10-05T22:39:30 | INFO | models.videochat_mistra.videochat2_it_mistral : Load VideoChat2 from: ./ckpt/stage2/videochat2_mistral_7b_stage2.pth
2024-10-05T22:39:31 | INFO | models.videochat_mistra.videochat2_it_mistral : _IncompatibleKeys(missing_keys=['qformer.bert.embeddings.word_embeddings.weight', 'qformer.bert.embeddings.position_embeddings.weight', 'qformer.bert.encoder.layer.0.intermediate.dense.weight', 'qformer.bert.encoder.layer.0.intermediate.dense.bias', 'qformer.bert.encoder.layer.0.output.dense.weight', 'qformer.bert.encoder.layer.0.output.dense.bias', 'qformer.bert.encoder.layer.0.output.LayerNorm.weight', 'qformer.bert.encoder.layer.0.output.LayerNorm.bias', 'qformer.bert.encoder.layer.1.intermediate.dense.weight', 'qformer.bert.encoder.layer.1.intermediate.dense.bias', 'qformer.bert.encoder.layer.1.output.dense.weight', 'qformer.bert.encoder.layer.1.output.dense.bias', 'qformer.bert.encoder.layer.1.output.LayerNorm.weight', 'qformer.bert.encoder.layer.1.output.LayerNorm.bias', 'qformer.bert.encoder.layer.2.intermediate.dense.weight', 'qformer.bert.encoder.layer.2.intermediate.dense.bias', 'qformer.bert.encoder.layer.2.output.dense.weight', 'qformer.bert.encoder.layer.2.output.dense.bias', 'qformer.bert.encoder.layer.2.output.LayerNorm.weight', 'qformer.bert.encoder.layer.2.output.LayerNorm.bias', 'qformer.bert.encoder.layer.3.intermediate.dense.weight', 'qformer.bert.encoder.layer.3.intermediate.dense.bias', 'qformer.bert.encoder.layer.3.output.dense.weight', 'qformer.bert.encoder.layer.3.output.dense.bias', 'qformer.bert.encoder.layer.3.output.LayerNorm.weight', 'qformer.bert.encoder.layer.3.output.LayerNorm.bias', 'qformer.bert.encoder.layer.4.intermediate.dense.weight', 'qformer.bert.encoder.layer.4.intermediate.dense.bias', 'qformer.bert.encoder.layer.4.output.dense.weight', 'qformer.bert.encoder.layer.4.output.dense.bias', 'qformer.bert.encoder.layer.4.output.LayerNorm.weight', 'qformer.bert.encoder.layer.4.output.LayerNorm.bias', 'qformer.bert.encoder.layer.5.intermediate.dense.weight', 'qformer.bert.encoder.layer.5.intermediate.dense.bias', 'qformer.bert.encoder.layer.5.output.dense.weight', 'qformer.bert.encoder.layer.5.output.dense.bias', 'qformer.bert.encoder.layer.5.output.LayerNorm.weight', 'qformer.bert.encoder.layer.5.output.LayerNorm.bias', 'qformer.bert.encoder.layer.6.intermediate.dense.weight', 'qformer.bert.encoder.layer.6.intermediate.dense.bias', 'qformer.bert.encoder.layer.6.output.dense.weight', 'qformer.bert.encoder.layer.6.output.dense.bias', 'qformer.bert.encoder.layer.6.output.LayerNorm.weight', 'qformer.bert.encoder.layer.6.output.LayerNorm.bias', 'qformer.bert.encoder.layer.7.intermediate.dense.weight', 'qformer.bert.encoder.layer.7.intermediate.dense.bias', 'qformer.bert.encoder.layer.7.output.dense.weight', 'qformer.bert.encoder.layer.7.output.dense.bias', 'qformer.bert.encoder.layer.7.output.LayerNorm.weight', 'qformer.bert.encoder.layer.7.output.LayerNorm.bias', 'qformer.bert.encoder.layer.8.intermediate.dense.weight', 'qformer.bert.encoder.layer.8.intermediate.dense.bias', 'qformer.bert.encoder.layer.8.output.dense.weight', 'qformer.bert.encoder.layer.8.output.dense.bias', 'qformer.bert.encoder.layer.8.output.LayerNorm.weight', 'qformer.bert.encoder.layer.8.output.LayerNorm.bias', 'qformer.bert.encoder.layer.9.intermediate.dense.weight', 'qformer.bert.encoder.layer.9.intermediate.dense.bias', 'qformer.bert.encoder.layer.9.output.dense.weight', 'qformer.bert.encoder.layer.9.output.dense.bias', 'qformer.bert.encoder.layer.9.output.LayerNorm.weight', 'qformer.bert.encoder.layer.9.output.LayerNorm.bias', 'qformer.bert.encoder.layer.10.intermediate.dense.weight', 'qformer.bert.encoder.layer.10.intermediate.dense.bias', 'qformer.bert.encoder.layer.10.output.dense.weight', 'qformer.bert.encoder.layer.10.output.dense.bias', 'qformer.bert.encoder.layer.10.output.LayerNorm.weight', 'qformer.bert.encoder.layer.10.output.LayerNorm.bias', 'qformer.bert.encoder.layer.11.intermediate.dense.weight', 'qformer.bert.encoder.layer.11.intermediate.dense.bias', 'qformer.bert.encoder.layer.11.output.dense.weight', 'qformer.bert.encoder.layer.11.output.dense.bias', 'qformer.bert.encoder.layer.11.output.LayerNorm.weight', 'qformer.bert.encoder.layer.11.output.LayerNorm.bias', 'mistral_model.base_model.model.model.embed_tokens.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.norm.weight', 'mistral_model.base_model.model.lm_head.weight', 'mistral_model.base_model.model.lm_head.lora_A.default.weight', 'mistral_model.base_model.model.lm_head.lora_B.default.weight'], unexpected_keys=[])
2024-10-05T22:44:42 | INFO | vindlu : Logging to: scripts/videochat_mistral/train.log
2024-10-05T22:44:42 | INFO | utils.config_utils : config: {
  anno_root_it: .
  available_corpus: {
      caption_HumanML3D: ['./HumanML3D/train.json', './HumanML3D', 'video']
      videochat2_instruction: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_new: [['./HumanML3D/train.json', './HumanML3D', 'video']]
      videochat2_instruction_hd: [['./HumanML3D/train.json', './HumanML3D', 'video']] }
  train_corpus: videochat2_instruction_new
  train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
  test_file: {

  test_types: []
  num_workers: 6
  stop_key: None
  num_frames: 20
  num_frames_test: 20
  batch_size: 1
  max_txt_l: 512
  pre_text: False
  inputs: {
      image_res: 224
      video_input: {
          num_frames: 20
          sample_type: rand
          num_frames_test: 20
          sample_type_test: middle
          random_aug: False }
      max_txt_l: {
          image: 512
          video: 512 }
      batch_size: {
          image: 1
          video: 1 }
      batch_size_test: {
          image: 1
          video: 1 } }
  model: {
      model_cls: VideoChat2_it_mistral
      vit_blip_model_path: ./UMT/umt_l16_qformer.pth
      mistral_model_path: ./Mistral
      videochat2_model_path: ./ckpt/stage2/videochat2_mistral_7b_stage2.pth
      freeze_vit: False
      freeze_qformer: False
      max_txt_len: 512
      low_resource: False
      vision_encoder: {
          name: vit_l14
          img_size: 224
          patch_size: 16
          d_model: 1024
          encoder_embed_dim: 1024
          encoder_depth: 24
          encoder_num_heads: 16
          drop_path_rate: 0.0
          num_frames: 20
          tubelet_size: 1
          use_checkpoint: True
          checkpoint_num: 18
          pretrained: 
          return_index: -2
          vit_add_ln: True
          ckpt_num_frame: 4 }
      num_query_token: 32
      qformer_hidden_dropout_prob: 0.1
      qformer_attention_probs_dropout_prob: 0.1
      qformer_drop_path_rate: 0.2
      extra_num_query_token: 64
      qformer_text_input: True
      system: 
      start_token: <Video>
      end_token: </Video>
      add_second_msg: True
      img_start_token: <Image>
      img_end_token: </Image>
      random_shuffle: True
      use_flash_attention: True
      use_lora: True
      lora_r: 16
      lora_alpha: 32
      lora_dropout: 0.1 }
  optimizer: {
      opt: adamW
      lr: 2e-05
      opt_betas: [0.9, 0.999]
      weight_decay: 0.02
      max_grad_norm: -1
      different_lr: {
          enable: False
          module_names: []
          lr: 0.001 } }
  scheduler: {
      sched: cosine
      epochs: 3
      min_lr_multi: 0.25
      warmup_epochs: 0.6 }
  evaluate: False
  deep_fusion: False
  evaluation: {
      eval_frame_ensemble: concat
      eval_x_only: False
      k_test: 128
      eval_offload: True }
  fp16: True
  gradient_checkpointing: True
  wandb: {
      enable: False
      entity: likunchang
      project: videochat2 }
  dist_url: env://
  device: cuda:0
  mode: it_mistral
  output_dir: scripts/videochat_mistral/
  resume: False
  debug: False
  log_freq: 10
  seed: 42
  save_latest: True
  auto_resume: True
  pretrained_path: 
  rank: 0
  world_size: 1
  gpu: 0
  distributed: True
  dist_backend: nccl }
2024-10-05T22:44:42 | INFO | __main__ : train_file: [['./HumanML3D/train.json', './HumanML3D', 'video']]
2024-10-05T22:44:42 | INFO | __main__ : Creating dataset for it_mistral
2024-10-05T22:44:42 | INFO | dataset.it_dataset_mistral : Load json file
2024-10-05T22:44:42 | INFO | dataset.it_dataset_mistral : Random shuffle: True
2024-10-05T22:44:42 | INFO | dataset.it_dataset_mistral : Return question with instruction: False
2024-10-05T22:44:42 | INFO | dataset.it_dataset_mistral : Use decord for data in ['./HumanML3D/train.json', './HumanML3D', 'video']
2024-10-05T22:44:42 | INFO | tasks.shared_utils : Creating model
2024-10-05T22:44:42 | INFO | models.videochat_mistra.videochat2_it_mistral : Add instruction in qformer: True
2024-10-05T22:44:42 | INFO | models.blip2.vit : Num of patches: 3920
2024-10-05T22:44:42 | INFO | models.blip2.vit : Use checkpoint: True
2024-10-05T22:44:42 | INFO | models.blip2.vit : Checkpoint number: 18
2024-10-05T22:44:42 | INFO | models.blip2.vit : Real runing depth: 23
2024-10-05T22:44:42 | INFO | models.blip2.vit : Interpolate position embedding
2024-10-05T22:44:42 | INFO | models.blip2.vit : Testing frame: 20
2024-10-05T22:44:42 | INFO | models.blip2.vit : Checkpoint frame: 4
2024-10-05T22:44:45 | INFO | models.blip2.vit : With LN: False
2024-10-05T22:44:45 | INFO | models.blip2.vit : Total 24 layer
2024-10-05T22:44:45 | INFO | models.blip2.vit : Return 23-th layer
2024-10-05T22:44:46 | INFO | models.blip2.vit : No pretrained weights!!!
2024-10-05T22:44:46 | INFO | models.blip2.blip2 : Drop_path:[0.0, 0.0181818176060915, 0.036363635212183, 0.05454545468091965, 0.072727270424366, 0.09090908616781235, 0.10909091681241989, 0.12727272510528564, 0.1454545557498932, 0.16363637149333954, 0.1818181872367859, 0.20000000298023224]
2024-10-05T22:44:46 | INFO | models.blip2.blip2 : BertConfig {
  "add_cross_attention": true,
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "cross_attention_freq": 2,
  "drop_path_list": [
    0.0,
    0.0181818176060915,
    0.036363635212183,
    0.05454545468091965,
    0.072727270424366,
    0.09090908616781235,
    0.10909091681241989,
    0.12727272510528564,
    0.1454545557498932,
    0.16363637149333954,
    0.1818181872367859,
    0.20000000298023224
  ],
  "encoder_width": 1024,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "query_length": 32,
  "transformers_version": "4.40.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

2024-10-05T22:44:48 | INFO | models.videochat_mistra.videochat2_it_mistral : Load ViT and QFormer from ./UMT/umt_l16_qformer.pth
2024-10-05T22:44:48 | INFO | models.videochat_mistra.videochat2_it_mistral : _IncompatibleKeys(missing_keys=[], unexpected_keys=['vision_temp_embed', 'temp', 'vision_proj.weight', 'vision_proj.bias', 'text_proj.weight', 'text_proj.bias', 'itm_head.weight', 'itm_head.bias'])
2024-10-05T22:44:48 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading ViT and Q-Former Done
2024-10-05T22:44:48 | INFO | models.videochat_mistra.videochat2_it_mistral : Add extra 64 tokens in QFormer
2024-10-05T22:44:48 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading Mistral
2024-10-05T22:44:48 | INFO | models.videochat_mistra.videochat2_it_mistral : Set pad_token
2024-10-05T22:44:50 | INFO | models.videochat_mistra.videochat2_it_mistral : freeze Mistral
2024-10-05T22:44:50 | INFO | models.videochat_mistra.videochat2_it_mistral : Loading Mistral Done
2024-10-05T22:44:50 | INFO | models.videochat_mistra.videochat2_it_mistral : Use lora
2024-10-05T22:45:52 | INFO | models.videochat_mistra.videochat2_it_mistral : Load VideoChat2 from: ./ckpt/stage2/videochat2_mistral_7b_stage2.pth
2024-10-05T22:45:52 | INFO | models.videochat_mistra.videochat2_it_mistral : _IncompatibleKeys(missing_keys=['qformer.bert.embeddings.word_embeddings.weight', 'qformer.bert.embeddings.position_embeddings.weight', 'qformer.bert.encoder.layer.0.intermediate.dense.weight', 'qformer.bert.encoder.layer.0.intermediate.dense.bias', 'qformer.bert.encoder.layer.0.output.dense.weight', 'qformer.bert.encoder.layer.0.output.dense.bias', 'qformer.bert.encoder.layer.0.output.LayerNorm.weight', 'qformer.bert.encoder.layer.0.output.LayerNorm.bias', 'qformer.bert.encoder.layer.1.intermediate.dense.weight', 'qformer.bert.encoder.layer.1.intermediate.dense.bias', 'qformer.bert.encoder.layer.1.output.dense.weight', 'qformer.bert.encoder.layer.1.output.dense.bias', 'qformer.bert.encoder.layer.1.output.LayerNorm.weight', 'qformer.bert.encoder.layer.1.output.LayerNorm.bias', 'qformer.bert.encoder.layer.2.intermediate.dense.weight', 'qformer.bert.encoder.layer.2.intermediate.dense.bias', 'qformer.bert.encoder.layer.2.output.dense.weight', 'qformer.bert.encoder.layer.2.output.dense.bias', 'qformer.bert.encoder.layer.2.output.LayerNorm.weight', 'qformer.bert.encoder.layer.2.output.LayerNorm.bias', 'qformer.bert.encoder.layer.3.intermediate.dense.weight', 'qformer.bert.encoder.layer.3.intermediate.dense.bias', 'qformer.bert.encoder.layer.3.output.dense.weight', 'qformer.bert.encoder.layer.3.output.dense.bias', 'qformer.bert.encoder.layer.3.output.LayerNorm.weight', 'qformer.bert.encoder.layer.3.output.LayerNorm.bias', 'qformer.bert.encoder.layer.4.intermediate.dense.weight', 'qformer.bert.encoder.layer.4.intermediate.dense.bias', 'qformer.bert.encoder.layer.4.output.dense.weight', 'qformer.bert.encoder.layer.4.output.dense.bias', 'qformer.bert.encoder.layer.4.output.LayerNorm.weight', 'qformer.bert.encoder.layer.4.output.LayerNorm.bias', 'qformer.bert.encoder.layer.5.intermediate.dense.weight', 'qformer.bert.encoder.layer.5.intermediate.dense.bias', 'qformer.bert.encoder.layer.5.output.dense.weight', 'qformer.bert.encoder.layer.5.output.dense.bias', 'qformer.bert.encoder.layer.5.output.LayerNorm.weight', 'qformer.bert.encoder.layer.5.output.LayerNorm.bias', 'qformer.bert.encoder.layer.6.intermediate.dense.weight', 'qformer.bert.encoder.layer.6.intermediate.dense.bias', 'qformer.bert.encoder.layer.6.output.dense.weight', 'qformer.bert.encoder.layer.6.output.dense.bias', 'qformer.bert.encoder.layer.6.output.LayerNorm.weight', 'qformer.bert.encoder.layer.6.output.LayerNorm.bias', 'qformer.bert.encoder.layer.7.intermediate.dense.weight', 'qformer.bert.encoder.layer.7.intermediate.dense.bias', 'qformer.bert.encoder.layer.7.output.dense.weight', 'qformer.bert.encoder.layer.7.output.dense.bias', 'qformer.bert.encoder.layer.7.output.LayerNorm.weight', 'qformer.bert.encoder.layer.7.output.LayerNorm.bias', 'qformer.bert.encoder.layer.8.intermediate.dense.weight', 'qformer.bert.encoder.layer.8.intermediate.dense.bias', 'qformer.bert.encoder.layer.8.output.dense.weight', 'qformer.bert.encoder.layer.8.output.dense.bias', 'qformer.bert.encoder.layer.8.output.LayerNorm.weight', 'qformer.bert.encoder.layer.8.output.LayerNorm.bias', 'qformer.bert.encoder.layer.9.intermediate.dense.weight', 'qformer.bert.encoder.layer.9.intermediate.dense.bias', 'qformer.bert.encoder.layer.9.output.dense.weight', 'qformer.bert.encoder.layer.9.output.dense.bias', 'qformer.bert.encoder.layer.9.output.LayerNorm.weight', 'qformer.bert.encoder.layer.9.output.LayerNorm.bias', 'qformer.bert.encoder.layer.10.intermediate.dense.weight', 'qformer.bert.encoder.layer.10.intermediate.dense.bias', 'qformer.bert.encoder.layer.10.output.dense.weight', 'qformer.bert.encoder.layer.10.output.dense.bias', 'qformer.bert.encoder.layer.10.output.LayerNorm.weight', 'qformer.bert.encoder.layer.10.output.LayerNorm.bias', 'qformer.bert.encoder.layer.11.intermediate.dense.weight', 'qformer.bert.encoder.layer.11.intermediate.dense.bias', 'qformer.bert.encoder.layer.11.output.dense.weight', 'qformer.bert.encoder.layer.11.output.dense.bias', 'qformer.bert.encoder.layer.11.output.LayerNorm.weight', 'qformer.bert.encoder.layer.11.output.LayerNorm.bias', 'mistral_model.base_model.model.model.embed_tokens.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.0.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.0.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.1.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.1.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.2.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.2.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.3.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.3.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.4.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.4.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.5.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.5.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.6.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.6.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.7.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.7.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.8.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.8.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.9.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.9.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.10.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.10.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.11.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.11.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.12.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.12.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.13.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.13.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.14.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.14.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.15.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.15.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.16.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.16.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.17.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.17.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.18.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.18.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.19.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.19.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.20.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.20.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.21.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.21.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.22.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.22.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.23.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.23.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.24.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.24.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.25.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.25.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.26.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.26.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.27.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.27.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.28.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.28.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.29.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.29.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.30.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.30.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.q_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.k_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.v_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.o_proj.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.gate_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.up_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.down_proj.weight', 'mistral_model.base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight', 'mistral_model.base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight', 'mistral_model.base_model.model.model.layers.31.input_layernorm.weight', 'mistral_model.base_model.model.model.layers.31.post_attention_layernorm.weight', 'mistral_model.base_model.model.model.norm.weight', 'mistral_model.base_model.model.lm_head.weight', 'mistral_model.base_model.model.lm_head.lora_A.default.weight', 'mistral_model.base_model.model.lm_head.lora_B.default.weight'], unexpected_keys=[])
2024-10-05T22:45:55 | INFO | utils.optimizer : diff_names: [], diff_lr: None
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.query_tokens: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.extra_query_tokens: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.patch_embed.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.0.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.1.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.2.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.3.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.4.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.5.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.6.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.7.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.8.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.9.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.10.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.11.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.12.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.13.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.14.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.15.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.16.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.17.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.18.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.19.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.20.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.21.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.norm1.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.norm1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.attn.q_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.attn.v_bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.attn.qkv.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.attn.proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.attn.proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.norm2.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.norm2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.mlp.fc1.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.mlp.fc1.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.mlp.fc2.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_encoder.encoder.blocks.22.mlp.fc2.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_layernorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.vision_layernorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.embeddings.word_embeddings.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.embeddings.position_embeddings.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.embeddings.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.embeddings.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.0.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.1.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.2.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.3.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.4.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.5.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.6.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.7.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.8.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.9.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.crossattention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.10.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.query.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.query.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.key.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.key.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.value.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.self.value.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.attention.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.intermediate.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.intermediate.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.intermediate_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.intermediate_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output_query.dense.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output_query.dense.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output_query.LayerNorm.weight: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.qformer.bert.encoder.layer.11.output_query.LayerNorm.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:55 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.lm_head.lora_A.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_model.base_model.model.lm_head.lora_B.default.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_proj.weight: wd: 0.02, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : param module.mistral_proj.bias: wd: 0, lr: 2e-05
2024-10-05T22:45:56 | INFO | utils.optimizer : optimizer -- lr=2e-05 wd=0.02 len(p)=668
2024-10-05T22:45:56 | INFO | utils.optimizer : optimizer -- lr=2e-05 wd=0 len(p)=417
2024-10-05T22:45:56 | INFO | tasks.shared_utils : Auto resuming
2024-10-05T22:45:56 | INFO | tasks.shared_utils : Not found checkpoint in scripts/videochat_mistral/
2024-10-05T22:45:56 | WARNING | tasks.shared_utils : No pretrained checkpoint provided, training from scratch
2024-10-05T22:45:56 | INFO | __main__ : Start training
2024-10-05T22:45:56 | INFO | dataset.dataloader : MetaLoader has 1 dataloaders, 22 batches in total
dataloader index=0 name=video, batch-size=1 length(#batches)=22 
2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

2024-10-05T22:45:56 | WARNING | py.warnings : /home/shanlins/videochat2_finetune/utils/distributed.py:18: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.
  builtin_warn(*args, **kwargs)

